apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: {{ .Values.model.displayName }}
    serving.kserve.io/deploymentMode: RawDeployment
  name: {{ .Values.model.name }}
  labels:
    {{- include "llama-guard.labels" . | nindent 4 }}
spec:
  predictor:
    {{- if .Values.inferenceService.automountServiceAccountToken }}
    automountServiceAccountToken: {{ .Values.inferenceService.automountServiceAccountToken }}
    {{- else }}
    automountServiceAccountToken: false
    {{- end }}
    maxReplicas: {{ .Values.inferenceService.maxReplicas }}
    minReplicas: {{ .Values.inferenceService.minReplicas }}
    model:
      {{- with .Values.inferenceService.modelArgs }}
      args:
        {{- toYaml . | nindent 10 }}
      {{- end }}
      modelFormat:
        name: vLLM
      name: ''
      resources:
        {{- toYaml .Values.inferenceService.resources | nindent 8 }}
      runtime: {{ .Values.model.name }}
      storageUri: {{ .Values.model.storageUri | quote }}
    tolerations:
    {{- if .Values.inferenceService.buCluster }}
      - effect: NoSchedule
        key: nvidia.com/gpu
        value: Tesla-T4-SHARED
    {{- else }}
      {{- toYaml .Values.inferenceService.tolerations | nindent 6 }}
    {{- end }}
