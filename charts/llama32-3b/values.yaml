# Llama 3.2 3B Model Serving Configuration

# Model configuration
model:
  name: llama-32
  displayName: llama-32
  servedModelName: llama32
  storageUri: "oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct"

# ServingRuntime configuration
servingRuntime:
  image: "quay.io/modh/vllm:rhoai-2.19-cuda"
  port: 8080
  portName: http1

  # Built-in adapter configuration
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000

  # vLLM arguments
  vllm:
    dtype: "half"
    gpuMemoryUtilization: "0.95"
    maxModelLen: "20000"
    enableAutoToolChoice: true
    enableChunkedPrefill: true
    toolCallParser: llama3_json
    chatTemplate: /app/data/template/tool_chat_template_llama3.2_json.jinja

  # Environment variables
  env:
    - name: VLLM_CONFIG_ROOT
      value: /tmp

  # Shared memory configuration
  shm:
    sizeLimit: 2Gi

# InferenceService configuration
inferenceService:
  minReplicas: 0
  maxReplicas: 1

  # Model format
  modelFormat: pytorch

  # Additional model args
  modelArgs:
    - '--enable-auto-tool-choice'
    - '--tool-call-parser=llama3_json'

  # Resource limits and requests
  resources:
    limits:
      cpu: "4"
      memory: 20Gi
      nvidia.com/gpu: "1"
    requests:
      cpu: "1"
      memory: 8Gi
      nvidia.com/gpu: "1"

  # GPU tolerations
  # Set to true for BU cluster with Tesla-T4-PRIVATE GPUs
  buCluster: false

  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"
      value: "True"
      effect: "NoSchedule"

# OCI Data Connection Secret
dataConnection:
  create: false
  name: llama-32-3b-instruct
  displayName: llama-3.2-3b-instruct
  description: ""
  # Base64 encoded URI (will be auto-encoded from model.storageUri if not set)
  uri: ""

# Route configuration (OpenShift only)
route:
  create: false
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect

# OpenDataHub labels
labels:
  dashboard: true
